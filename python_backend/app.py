import torch
import json
import librosa
import numpy as np
import os
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
import tempfile

# Load labels
with open("labels.json") as f:
    LABELS = json.load(f)

# Load model
try:
    model_state = torch.load("model_best.pth", map_location="cpu")
    
    # Create CNN model architecture that matches your Huawei training
    import torch.nn as nn
    class HuaweiCNNModel(nn.Module):
        def __init__(self, num_classes=3):
            super().__init__()
            self.net = nn.Sequential(
                nn.Conv2d(1, 16, 3, padding=1),  # First conv layer
                nn.BatchNorm2d(16),
                nn.ReLU(),
                nn.MaxPool2d(2),
                nn.Conv2d(16, 32, 3, padding=1),  # Second conv layer
                nn.BatchNorm2d(32),
                nn.ReLU(),
                nn.MaxPool2d(2),
                nn.Conv2d(32, 64, 3, padding=1),  # Third conv layer
                nn.BatchNorm2d(64),
                nn.ReLU(),
                nn.AdaptiveAvgPool2d((1, 1))
            )
            self.head = nn.Linear(64, num_classes)
        
        def forward(self, x):
            # Reshape MFCC features to image-like format
            if len(x.shape) == 2:  # [batch, features]
                x = x.view(x.size(0), 1, 8, 5)  # Reshape 40 features to 8x5
            x = self.net(x)
            x = x.view(x.size(0), -1)  # Flatten
            return self.head(x)
    
    model = HuaweiCNNModel()
    model.load_state_dict(model_state)
    model.eval()
    print("‚úÖ Huawei CNN ModelArts model loaded successfully!")
except Exception as e:
    print(f"‚ùå Error loading CNN model: {e}")
    print("Creating dummy model for testing...")
    
    # Fallback dummy model
    import torch.nn as nn
    class DummyModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.fc = nn.Linear(40, 3)
        
        def forward(self, x):
            return self.fc(x)
    
    model = DummyModel()
    print("‚úÖ Dummy model created for testing")

app = FastAPI(title="SafeSound AI - Huawei ModelArts Integration")

# Enable CORS for web interface
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

def extract_features(wav_path):
    """Extract MFCC features from audio file"""
    try:
        y, sr = librosa.load(wav_path, sr=16000)
        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)
        return np.mean(mfcc.T, axis=0)
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Audio processing error: {str(e)}")

def get_risk_level(emotion, confidence):
    """Determine risk level based on emotion and confidence"""
    high_risk = ["baby_crying", "adult_crying"]
    
    if emotion in high_risk and confidence > 0.7:
        return "danger"
    elif emotion in high_risk and confidence > 0.5:
        return "warning"
    else:
        return "safe"

@app.get("/")
async def root():
    return {
        "message": "SafeSound AI - Huawei ModelArts Integration",
        "model_loaded": model is not None,
        "labels": LABELS
    }

@app.post("/predict")
async def predict(file: UploadFile = File(...)):
    print(f"üìÅ Received file: {file.filename}, type: {file.content_type}, size: {file.size if hasattr(file, 'size') else 'unknown'}")
    
    if model is None:
        raise HTTPException(status_code=500, detail="Model not loaded")
    
    # Save uploaded file temporarily
    with tempfile.NamedTemporaryFile(delete=False, suffix=".webm") as temp_file:
        content = await file.read()
        temp_file.write(content)
        temp_path = temp_file.name
        print(f"üíæ Saved temp file: {temp_path}, size: {len(content)} bytes")
    
    try:
        # Extract features (librosa can handle webm/ogg format)
        print("üéµ Extracting MFCC features...")
        features = extract_features(temp_path)
        print(f"‚úÖ Features extracted: shape {features.shape}")
        
        x = torch.tensor(features).float().unsqueeze(0)
        print(f"üî¢ Input tensor shape: {x.shape}")
        
        # Make prediction
        print("ü§ñ Making prediction with Huawei model...")
        with torch.no_grad():
            outputs = model(x)
            probs = torch.softmax(outputs, dim=1)
            conf, pred = torch.max(probs, 1)
        
        # Get emotion label
        emotion = list(LABELS.keys())[pred.item()]  # Fix label mapping
        confidence = float(conf.item())
        risk_level = get_risk_level(emotion, confidence)
        
        print(f"üéØ Prediction: {emotion} ({confidence:.2f} confidence)")
        
        # Generate AI reasoning
        reasoning = generate_reasoning(emotion, confidence)
        
        result = {
            "emotion": emotion,
            "confidence": confidence,
            "risk_level": risk_level,
            "reasoning": reasoning,
            "model_source": "Huawei ModelArts",
            "timestamp": str(np.datetime64('now'))
        }
        
        print(f"üì§ Returning result: {result}")
        return result
        
    except Exception as e:
        print(f"‚ùå Prediction error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Prediction error: {str(e)}")
    
    finally:
        # Clean up temp file
        if os.path.exists(temp_path):
            os.unlink(temp_path)
            print(f"üóëÔ∏è Cleaned up temp file")

def generate_reasoning(emotion, confidence):
    """Generate AI reasoning based on prediction"""
    reasonings = {
        "adult_crying": f"Detected adult emotional distress with {confidence:.1%} confidence. Voice patterns indicate potential crisis situation requiring immediate attention.",
        "baby_crying": f"Identified infant distress signals with {confidence:.1%} confidence. High-pitched vocalizations consistent with baby crying detected.",
        "adult_laughing": f"Normal adult laughter detected with {confidence:.1%} confidence. Positive emotional state, no safety concerns identified."
    }
    
    return reasonings.get(emotion, f"Audio classified as {emotion} with {confidence:.1%} confidence using Huawei ModelArts trained model.")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)